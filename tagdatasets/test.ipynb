{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:44:25.102218Z",
     "start_time": "2025-08-01T07:44:25.098347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "a04dab5eea0365ac",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:41:41.211611Z",
     "start_time": "2025-08-01T07:41:02.798742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to download the Cora dataset from available sources\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download a file from URL\"\"\"\n",
    "    print(f\"Downloading from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "def setup_cora_planetoid():\n",
    "    \"\"\"Download Cora from Planetoid (primary source)\"\"\"\n",
    "    print(\"\\nAttempting to download Cora dataset from Planetoid format...\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(\"datasets/cora_orig\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Download from GitHub mirror of Planetoid datasets\n",
    "        base_url = \"https://raw.githubusercontent.com/kimiyoung/planetoid/master/data\"\n",
    "\n",
    "        files_to_download = [\n",
    "            (\"ind.cora.x\", \"cora.x\"),\n",
    "            (\"ind.cora.y\", \"cora.y\"),\n",
    "            (\"ind.cora.tx\", \"cora.tx\"),\n",
    "            (\"ind.cora.ty\", \"cora.ty\"),\n",
    "            (\"ind.cora.allx\", \"cora.allx\"),\n",
    "            (\"ind.cora.ally\", \"cora.ally\"),\n",
    "            (\"ind.cora.graph\", \"cora.graph\"),\n",
    "            (\"ind.cora.test.index\", \"cora.test.index\")\n",
    "        ]\n",
    "\n",
    "        temp_dir = \"temp_cora_download\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        # Download all files\n",
    "        for remote_file, local_file in files_to_download:\n",
    "            url = f\"{base_url}/{remote_file}\"\n",
    "            download_file(url, os.path.join(temp_dir, local_file))\n",
    "\n",
    "        print(\"\\nConverting Planetoid format to standard Cora format...\")\n",
    "\n",
    "        # Convert to standard format\n",
    "        convert_planetoid_to_standard(temp_dir, \"datasets/cora_orig\")\n",
    "\n",
    "        # Clean up\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "        print(\"✓ Successfully downloaded and converted Cora dataset!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download from Planetoid: {e}\")\n",
    "        return False\n",
    "\n",
    "def convert_planetoid_to_standard(input_dir, output_dir):\n",
    "    \"\"\"Convert Planetoid format to standard Cora format\"\"\"\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    # Load data\n",
    "    with open(os.path.join(input_dir, \"cora.x\"), 'rb') as f:\n",
    "        x = pickle.load(f, encoding='latin1')\n",
    "    with open(os.path.join(input_dir, \"cora.y\"), 'rb') as f:\n",
    "        y = pickle.load(f, encoding='latin1')\n",
    "    with open(os.path.join(input_dir, \"cora.tx\"), 'rb') as f:\n",
    "        tx = pickle.load(f, encoding='latin1')\n",
    "    with open(os.path.join(input_dir, \"cora.ty\"), 'rb') as f:\n",
    "        ty = pickle.load(f, encoding='latin1')\n",
    "    with open(os.path.join(input_dir, \"cora.allx\"), 'rb') as f:\n",
    "        allx = pickle.load(f, encoding='latin1')\n",
    "    with open(os.path.join(input_dir, \"cora.ally\"), 'rb') as f:\n",
    "        ally = pickle.load(f, encoding='latin1')\n",
    "    with open(os.path.join(input_dir, \"cora.graph\"), 'rb') as f:\n",
    "        graph = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    # Read test indices\n",
    "    test_idx_reorder = []\n",
    "    with open(os.path.join(input_dir, \"cora.test.index\"), 'r') as f:\n",
    "        for line in f:\n",
    "            test_idx_reorder.append(int(line.strip()))\n",
    "\n",
    "    # Process features\n",
    "    features = np.vstack((allx, tx))\n",
    "    features[test_idx_reorder, :] = features[len(allx):, :]\n",
    "    features = features[:len(allx), :]\n",
    "\n",
    "    # Process labels\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[len(ally):, :]\n",
    "    labels = labels[:len(ally), :]\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "\n",
    "    # Class names\n",
    "    class_names = ['Case_Based', 'Genetic_Algorithms', 'Neural_Networks',\n",
    "                   'Probabilistic_Methods', 'Reinforcement_Learning',\n",
    "                   'Rule_Learning', 'Theory']\n",
    "\n",
    "    # Create content file\n",
    "    content_file = os.path.join(output_dir, \"cora.content\")\n",
    "    with open(content_file, 'w') as f:\n",
    "        for i in range(features.shape[0]):\n",
    "            # Node ID\n",
    "            f.write(f\"{i}\\t\")\n",
    "            # Features\n",
    "            feature_str = '\\t'.join([str(int(x)) for x in features[i]])\n",
    "            f.write(f\"{feature_str}\\t\")\n",
    "            # Class label\n",
    "            f.write(f\"{class_names[labels[i]]}\\n\")\n",
    "\n",
    "    # Create cites file\n",
    "    cites_file = os.path.join(output_dir, \"cora.cites\")\n",
    "    edges = set()\n",
    "    for node, neighbors in graph.items():\n",
    "        for neighbor in neighbors:\n",
    "            edges.add((node, neighbor))\n",
    "\n",
    "    with open(cites_file, 'w') as f:\n",
    "        for src, dst in sorted(edges):\n",
    "            f.write(f\"{src}\\t{dst}\\n\")\n",
    "\n",
    "    print(f\"Created {content_file} and {cites_file}\")\n",
    "\n",
    "def download_cora_alternative():\n",
    "    \"\"\"Alternative: Download from other sources\"\"\"\n",
    "    print(\"\\nTrying alternative download source...\")\n",
    "\n",
    "    try:\n",
    "        # Alternative URL (from LINQS)\n",
    "        url = \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\"\n",
    "\n",
    "        os.makedirs(\"datasets\", exist_ok=True)\n",
    "        temp_file = \"cora.tgz\"\n",
    "\n",
    "        download_file(url, temp_file)\n",
    "\n",
    "        # Extract\n",
    "        print(\"Extracting files...\")\n",
    "        with tarfile.open(temp_file, 'r:gz') as tar:\n",
    "            tar.extractall(\"datasets\")\n",
    "\n",
    "        # Rename directory if needed\n",
    "        if os.path.exists(\"datasets/cora\"):\n",
    "            os.rename(\"datasets/cora\", \"datasets/cora_orig\")\n",
    "\n",
    "        # Clean up\n",
    "        os.remove(temp_file)\n",
    "\n",
    "        print(\"✓ Successfully downloaded Cora dataset from alternative source!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download from alternative source: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify that the dataset was downloaded correctly\"\"\"\n",
    "    required_files = [\n",
    "        \"datasets/cora_orig/cora.content\",\n",
    "        \"datasets/cora_orig/cora.cites\"\n",
    "    ]\n",
    "\n",
    "    all_present = True\n",
    "    for file in required_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"✓ {file} exists\")\n",
    "        else:\n",
    "            print(f\"✗ {file} is missing\")\n",
    "            all_present = False\n",
    "\n",
    "    if all_present:\n",
    "        # Check file contents\n",
    "        with open(\"datasets/cora_orig/cora.content\", 'r') as f:\n",
    "            num_nodes = sum(1 for _ in f)\n",
    "        with open(\"datasets/cora_orig/cora.cites\", 'r') as f:\n",
    "            num_edges = sum(1 for _ in f)\n",
    "\n",
    "        print(f\"\\nDataset statistics:\")\n",
    "        print(f\"- Number of nodes: {num_nodes}\")\n",
    "        print(f\"- Number of edges: {num_edges}\")\n",
    "\n",
    "    return all_present\n",
    "\n",
    "def main():\n",
    "    print(\"Cora Dataset Downloader\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check if dataset already exists\n",
    "    if os.path.exists(\"datasets/cora_orig/cora.content\") and \\\n",
    "       os.path.exists(\"datasets/cora_orig/cora.cites\"):\n",
    "        print(\"Dataset already exists!\")\n",
    "        verify_dataset()\n",
    "        return\n",
    "\n",
    "    # Try downloading from different sources\n",
    "    success = setup_cora_planetoid()\n",
    "\n",
    "    if not success:\n",
    "        success = download_cora_alternative()\n",
    "\n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Verifying dataset...\")\n",
    "        verify_dataset()\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"Failed to download Cora dataset from all sources.\")\n",
    "        print(\"\\nManual download instructions:\")\n",
    "        print(\"1. Visit: https://linqs.soe.ucsc.edu/data\")\n",
    "        print(\"2. Download the Cora dataset\")\n",
    "        print(\"3. Extract to ./datasets/cora_orig/\")\n",
    "        print(\"4. Ensure you have cora.content and cora.cites files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "20a4e5a877b212a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora Dataset Downloader\n",
      "==================================================\n",
      "\n",
      "Attempting to download Cora dataset from Planetoid format...\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.x...\n",
      "Downloaded temp_cora_download/cora.x\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.y...\n",
      "Downloaded temp_cora_download/cora.y\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.tx...\n",
      "Downloaded temp_cora_download/cora.tx\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.ty...\n",
      "Downloaded temp_cora_download/cora.ty\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.allx...\n",
      "Downloaded temp_cora_download/cora.allx\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.ally...\n",
      "Downloaded temp_cora_download/cora.ally\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.graph...\n",
      "Downloaded temp_cora_download/cora.graph\n",
      "Downloading from https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.test.index...\n",
      "Downloaded temp_cora_download/cora.test.index\n",
      "\n",
      "Converting Planetoid format to standard Cora format...\n",
      "Failed to download from Planetoid: sparse array length is ambiguous; use getnnz() or shape[0]\n",
      "\n",
      "Trying alternative download source...\n",
      "Downloading from https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20726/4143103383.py:75: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  x = pickle.load(f, encoding='latin1')\n",
      "/tmp/ipykernel_20726/4143103383.py:79: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  tx = pickle.load(f, encoding='latin1')\n",
      "/tmp/ipykernel_20726/4143103383.py:83: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  allx = pickle.load(f, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded cora.tgz\n",
      "Extracting files...\n",
      "✓ Successfully downloaded Cora dataset from alternative source!\n",
      "\n",
      "==================================================\n",
      "Verifying dataset...\n",
      "✓ datasets/cora_orig/cora.content exists\n",
      "✓ datasets/cora_orig/cora.cites exists\n",
      "\n",
      "Dataset statistics:\n",
      "- Number of nodes: 2708\n",
      "- Number of edges: 5429\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:27:57.628001Z",
     "start_time": "2025-08-01T07:27:57.623753Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "857b00857ec157b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:27:59.724358Z",
     "start_time": "2025-08-01T07:27:59.664173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Planetoid('./datasets', 'cora',\n",
    "                        transform=T.NormalizeFeatures())\n",
    "data = dataset[0]"
   ],
   "id": "984e6cc456b95eca",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:28:04.888797Z",
     "start_time": "2025-08-01T07:28:04.883900Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "d47d70d06a484a5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:34:51.828134Z",
     "start_time": "2025-08-01T07:34:51.324019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_cora():\n",
    "    path = './datasets/cora_orig/cora'\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = idx_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = idx_features_labels[:, -1]\n",
    "    class_map = {x: i for i, x in enumerate(['Case_Based', 'Genetic_Algorithms', 'Neural_Networks',\n",
    "                                            'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory'])}\n",
    "    data_Y = np.array([class_map[l] for l in labels])\n",
    "    data_citeid = idx_features_labels[:, 0]\n",
    "    idx = np.array(data_citeid, dtype=np.dtype(str))\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\n",
    "        \"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten()))).reshape(\n",
    "        edges_unordered.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype='int')\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    return data_X, data_Y, data_citeid, np.unique(data_edges, axis=0).transpose()\n",
    "\n",
    "parse_cora()\n"
   ],
   "id": "b177d070bd88a18d",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./datasets/cora_orig/cora.content not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     18\u001B[39m     data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n\u001B[32m     19\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m data_X, data_Y, data_citeid, np.unique(data_edges, axis=\u001B[32m0\u001B[39m).transpose()\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43mparse_cora\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m, in \u001B[36mparse_cora\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mparse_cora\u001B[39m():\n\u001B[32m      2\u001B[39m     path = \u001B[33m'\u001B[39m\u001B[33m./datasets/cora_orig/cora\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     idx_features_labels = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenfromtxt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[33;43m.content\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     data_X = idx_features_labels[:, \u001B[32m1\u001B[39m:-\u001B[32m1\u001B[39m].astype(np.float32)\n\u001B[32m      6\u001B[39m     labels = idx_features_labels[:, -\u001B[32m1\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AmirKabir-Projects/.venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1990\u001B[39m, in \u001B[36mgenfromtxt\u001B[39m\u001B[34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001B[39m\n\u001B[32m   1988\u001B[39m     fname = os.fspath(fname)\n\u001B[32m   1989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1990\u001B[39m     fid = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_datasource\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1991\u001B[39m     fid_ctx = contextlib.closing(fid)\n\u001B[32m   1992\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AmirKabir-Projects/.venv/lib/python3.12/site-packages/numpy/lib/_datasource.py:192\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(path, mode, destpath, encoding, newline)\u001B[39m\n\u001B[32m    155\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    156\u001B[39m \u001B[33;03mOpen `path` with `mode` and return the file object.\u001B[39;00m\n\u001B[32m    157\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    188\u001B[39m \n\u001B[32m    189\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    191\u001B[39m ds = DataSource(destpath)\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mds\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AmirKabir-Projects/.venv/lib/python3.12/site-packages/numpy/lib/_datasource.py:529\u001B[39m, in \u001B[36mDataSource.open\u001B[39m\u001B[34m(self, path, mode, encoding, newline)\u001B[39m\n\u001B[32m    526\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _file_openers[ext](found, mode=mode,\n\u001B[32m    527\u001B[39m                               encoding=encoding, newline=newline)\n\u001B[32m    528\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m529\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not found.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: ./datasets/cora_orig/cora.content not found."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-31T15:09:27.088366Z",
     "start_time": "2025-07-31T15:09:26.698861Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# return cora dataset as pytorch geometric Data object together with 60/20/20 split, and list of cora IDs\n",
    "\n",
    "\n",
    "def get_cora_casestudy(SEED=0):\n",
    "    data_X, data_Y, data_citeid, data_edges = parse_cora()\n",
    "    # data_X = sklearn.preprocessing.normalize(data_X, norm=\"l1\")\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module.\n",
    "\n",
    "    # load data\n",
    "    data_name = 'cora'\n",
    "    # path = osp.join(osp.dirname(osp.realpath(__file__)), 'dataset')\n",
    "    dataset = Planetoid('./datasets', data_name,\n",
    "                        transform=T.NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "\n",
    "    data.x = torch.tensor(data_X).float()\n",
    "    data.edge_index = torch.tensor(data_edges).long()\n",
    "    data.y = torch.tensor(data_Y).long()\n",
    "    data.num_nodes = len(data_Y)\n",
    "\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    # data.train_id = np.sort(node_id[:int(data.num_nodes * 0.1)])\n",
    "    # data.val_id = np.sort(\n",
    "    #     node_id[int(data.num_nodes * 0.1):int(data.num_nodes * 0.2)])\n",
    "    # data.test_id = np.sort(node_id[int(data.num_nodes * 0.2):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "\n",
    "    return data, data_citeid\n",
    "\n",
    "# credit: https://github.com/tkipf/pygcn/issues/27, xuhaiyun\n",
    "\n",
    "\n",
    "def parse_cora():\n",
    "    path = './datasets/cora_orig/cora'\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = idx_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = idx_features_labels[:, -1]\n",
    "    class_map = {x: i for i, x in enumerate(['Case_Based', 'Genetic_Algorithms', 'Neural_Networks',\n",
    "                                            'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory'])}\n",
    "    data_Y = np.array([class_map[l] for l in labels])\n",
    "    data_citeid = idx_features_labels[:, 0]\n",
    "    idx = np.array(data_citeid, dtype=np.dtype(str))\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\n",
    "        \"{}.cites\".format(path), dtype=np.dtype(str))\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten()))).reshape(\n",
    "        edges_unordered.shape)\n",
    "    data_edges = np.array(edges[~(edges == None).max(1)], dtype='int')\n",
    "    data_edges = np.vstack((data_edges, np.fliplr(data_edges)))\n",
    "    return data_X, data_Y, data_citeid, np.unique(data_edges, axis=0).transpose()\n",
    "\n",
    "\n",
    "def get_raw_text_cora(use_text=False, seed=0):\n",
    "    data, data_citeid = get_cora_casestudy(seed)\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "\n",
    "    with open('./datasets/cora_orig/mccallum/cora/papers')as f:\n",
    "        lines = f.readlines()\n",
    "    pid_filename = {}\n",
    "    for line in lines:\n",
    "        pid = line.split('\\t')[0]\n",
    "        fn = line.split('\\t')[1]\n",
    "        pid_filename[pid] = fn\n",
    "\n",
    "    path = './datasets/cora_orig/mccallum/cora/extractions/'\n",
    "    text = []\n",
    "    for pid in data_citeid:\n",
    "        fn = pid_filename[pid]\n",
    "        with open(path+fn) as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            if 'Title:' in line:\n",
    "                ti = line\n",
    "            if 'Abstract:' in line:\n",
    "                ab = line\n",
    "        text.append(ti+'\\n'+ab)\n",
    "    return data, text\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./datasets/cora_orig/cora.content not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mparse_cora\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# return cora dataset as pytorch geometric Data object together with 60/20/20 split, and list of cora IDs\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_cora_casestudy\u001B[39m(SEED=\u001B[32m0\u001B[39m):\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m, in \u001B[36mparse_cora\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mparse_cora\u001B[39m():\n\u001B[32m      2\u001B[39m     path = \u001B[33m'\u001B[39m\u001B[33m./datasets/cora_orig/cora\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     idx_features_labels = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenfromtxt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[33;43m.content\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     data_X = idx_features_labels[:, \u001B[32m1\u001B[39m:-\u001B[32m1\u001B[39m].astype(np.float32)\n\u001B[32m      6\u001B[39m     labels = idx_features_labels[:, -\u001B[32m1\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AmirKabir-Projects/.venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1990\u001B[39m, in \u001B[36mgenfromtxt\u001B[39m\u001B[34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001B[39m\n\u001B[32m   1988\u001B[39m     fname = os.fspath(fname)\n\u001B[32m   1989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1990\u001B[39m     fid = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_datasource\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1991\u001B[39m     fid_ctx = contextlib.closing(fid)\n\u001B[32m   1992\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AmirKabir-Projects/.venv/lib/python3.12/site-packages/numpy/lib/_datasource.py:192\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(path, mode, destpath, encoding, newline)\u001B[39m\n\u001B[32m    155\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    156\u001B[39m \u001B[33;03mOpen `path` with `mode` and return the file object.\u001B[39;00m\n\u001B[32m    157\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    188\u001B[39m \n\u001B[32m    189\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    191\u001B[39m ds = DataSource(destpath)\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mds\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/AmirKabir-Projects/.venv/lib/python3.12/site-packages/numpy/lib/_datasource.py:529\u001B[39m, in \u001B[36mDataSource.open\u001B[39m\u001B[34m(self, path, mode, encoding, newline)\u001B[39m\n\u001B[32m    526\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _file_openers[ext](found, mode=mode,\n\u001B[32m    527\u001B[39m                               encoding=encoding, newline=newline)\n\u001B[32m    528\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m529\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not found.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: ./datasets/cora_orig/cora.content not found."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c67e6b9f79634d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
